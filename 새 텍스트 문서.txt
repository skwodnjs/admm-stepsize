후기. ADMM의 경우, 특히 차원이 굉장히 큰 문제에서는 step size가 굉장히 큼. 문제에 따라서 step size의 (의미있는) 범위가 굉장히 달라짐. 어떤 문제에서는 0~30 사이에서 모든 경향을 다 보여주고 30 이후부터는 그냥 MAX_ITER 박아버리는 반면 어떤 문제에서는(특히 차원을 높히면) 0~100 으로도 부족한 경우도 있음. 학습을 통해 objective 함수를 추측해야 하는 베이지안 최적화는 이러한 문제가 학습에 큰 영향을 미칠 수 있음. 왜냐면 범위가 이상하면 (예를들어) objective 함수가 거의 다 MAX_ITER에 찍히는 경우가 생길 수 있고 이는 objective 함수가 거의 상수함수인가? 하는 착각을 하게끔 만들 수도 있음. 그래서 문제에 따라 이걸 바꿔야 하는건지 고민해봐야됨.

그리고, 차원이 높아지면 GD로 할 때 계산을 못함. 아무래도 수렴을 못하는것 같은데, 더 실험해볼 필요가 있을 듯.

두 경우 모두 어느 정도는 잘 작동하는 것을 확인함. 이것저것 사소한 디테일 부분을 확인해서 잘 작동하는지 확인해보고, 이후에 어떤 방식이 더 좋은지(정확도와 시간 등을 확인해보면서) 비교해보아야 할 듯.

일단 식을 사용하는게 저차원에서는 확실히 유리한 것으로 보임. seed=21758 의 예시를 보면, 확률적으로 objective 함수를 추측하는 베이지안 최적화의 특성 상 그래프가 실제와 다르게 그려질 수가 있는데, 하필 그 지점에 최적의 해가 위치해버림.

여러번 실험을 해봤는데, seed=25465 에서처럼 두 값이 찾아낸 최적의 theta에 차이가 많이 나더라도 iteration 을 비교해보면 큰 차이가 없음을 알 수 있음.